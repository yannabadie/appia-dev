import json
import os
import random

from google.cloud import pubsub_v1
from langchain.chains import LLMChain
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from textblob import TextBlob

from supabase import Client, create_client

# Load environment variables
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_SERVICE_ROLE = os.getenv("SUPABASE_SERVICE_ROLE")
GCP_SA_JSON = json.loads(os.getenv("GCP_SA_JSON", "{}"))
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

# Initialize Supabase client
supabase: Client = create_client(SUPABASE_URL, SUPABASE_SERVICE_ROLE)

# Initialize GCP Pub/Sub publisher with service account
publisher = pubsub_v1.PublisherClient.from_service_account_info(GCP_SA_JSON)

# LLM setup
llm = OpenAI(openai_api_key=OPENAI_API_KEY, temperature=0.7)


# Sentiment analysis function
def analyze_sentiment(text: str) -> float:
    blob = TextBlob(text)
    return blob.sentiment.polarity


# Quantum-inspired decision making (simulated superposition)
def quantum_decide(options: list, sentiment: float) -> str:
    # Simulate quantum randomness with bias based on sentiment
    weights = [
        1 + sentiment if i % 2 == 0 else 1 - sentiment for i in range(len(options))
    ]
    total = sum(weights)
    probs = [w / total for w in weights]
    return random.choices(options, weights=probs, k=1)[0]


# Self-improvement loop: Query Supabase for past decisions, optimize
def self_improve():
    response = supabase.table("decisions").select("*").execute()
    past_decisions = response.data
    if past_decisions:
        avg_sentiment = sum(
            analyze_sentiment(d["input"]) for d in past_decisions
        ) / len(past_decisions)
        # Proactive enhancement: Adjust LLM temperature based on average sentiment
        llm.temperature = max(0.1, min(1.0, 0.5 + avg_sentiment))
        print(f"Self-improved LLM temperature to {llm.temperature}")


# LLM routing chain with sentiment and quantum decision
prompt = PromptTemplate(
    input_variables=["input"], template="Route this query: {input} to the best model."
)
chain = LLMChain(llm=llm, prompt=prompt)


def route_and_process(user_input: str):
    sentiment = analyze_sentiment(user_input)
    routing_options = ["ModelA", "ModelB", "ModelC"]  # Example models
    chosen_model = quantum_decide(routing_options, sentiment)

    # Process with chosen model (simulated)
    result = chain.run(input=user_input)

    # Store in Supabase for memory
    supabase.table("decisions").insert(
        {
            "input": user_input,
            "sentiment": sentiment,
            "chosen_model": chosen_model,
            "output": result,
        }
    ).execute()

    # Publish to GCP Pub/Sub for cloud orchestration
    topic_path = publisher.topic_path("your-project-id", "jarvys-topic")
    publisher.publish(topic_path, data=result.encode("utf-8"))

    return result


# Main execution for JARVYS_AI local deployment
if __name__ == "__main__":
    self_improve()
    user_input = input("Enter query: ")
    output = route_and_process(user_input)
    print(f"Processed output: {output}")
    # Proactive suggestion: Generate enhancement code snippet
    enhancement = """
def new_feature():
    print("Enhanced with quantum entanglement simulation")
"""
    with open("enhancement.py", "w") as f:
        f.write(enhancement)
    print("Generated enhancement code for appIA push.")
