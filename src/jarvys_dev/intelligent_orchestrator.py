import sys

"""
Orchestrateur intelligent pour s√©lection automatique des mod√®les LLM.
Analyse la t√¢che et s√©lectionne le mod√®le optimal en temps r√©el.
"""

import json
import logging
import os
from dataclasses import dataclass
from datetime import datetime
from typing import Dict, List, Tuple

import requests

logger = logging.getLogger(__name__) = logging.getLogger(__name__)


@dataclass
class ModelCapabilities:
    """Capacit√©s et sp√©cialit√©s d'un mod√®le."""

    name: str
    provider: str
    reasoning_score: float  # 0-1
    creativity_score: float  # 0-1
    speed_score: float  # 0-1
    multimodal: bool
    max_tokens: int
    cost_per_1k_tokens: float
    specialties: List[str]
    last_updated: str


@dataclass
class TaskAnalysis:
    """Analyse d'une t√¢che pour s√©lection de mod√®le."""

    task_type: str
    complexity: float  # 0-1
    creativity_needed: float  # 0-1
    reasoning_needed: float  # 0-1
    speed_priority: float  # 0-1
    multimodal_needed: bool
    estimated_tokens: int


class IntelligentOrchestrator:
    """Orchestrateur intelligent pour s√©lection automatique de mod√®les."""

    def __init__(self):
        self.models_db: Dict[str, ModelCapabilities] = {}
        self.performance_history: List[Dict] = []
        self.last_update = None
        self._initialize_models_database()
        self._load_from_huggingface()

    def _initialize_models_database(self):
        """Initialise la base de donn√©es des mod√®les avec les"
        "derni√®res capabilities."""

        # Mod√®les de pointe disponibles (mis √† jour avec les derni√®res versions)
        self.models_db = {
            # OpenAI - Mod√®les les plus r√©cents
            "gpt-4o": ModelCapabilities(
                name="gpt-4o",
                provider="openai",
                reasoning_score=0.95,
                creativity_score=0.88,
                speed_score=0.85,
                multimodal=True,
                max_tokens=128000,
                cost_per_1k_tokens=0.01,
                specialties=["reasoning", "coding", "analysis", "multimodal"],
                last_updated=datetime.now().isoformat(),
            ),
            "o1-preview": ModelCapabilities(
                name="o1-preview",
                provider="openai",
                reasoning_score=0.98,  # Meilleur raisonnement
                creativity_score=0.82,
                speed_score=0.65,  # Plus lent mais plus pr√©cis
                multimodal=False,
                max_tokens=32768,
                cost_per_1k_tokens=0.015,
                specialties=[
                    "complex_reasoning",
                    "mathematics",
                    "scientific_analysis",
                ],
                last_updated=datetime.now().isoformat(),
            ),
            "gpt-4o-mini": ModelCapabilities(
                name="gpt-4o-mini",
                provider="openai",
                reasoning_score=0.88,
                creativity_score=0.85,
                speed_score=0.95,  # Tr√®s rapide
                multimodal=True,
                max_tokens=128000,
                cost_per_1k_tokens=0.0001,  # Tr√®s √©conomique
                specialties=["fast_tasks", "simple_coding", "chat"],
                last_updated=datetime.now().isoformat(),
            ),
            # Anthropic Claude 4 - Nouvelles versions 2025
            "claude-opus-4-20250514": ModelCapabilities(
                name="claude-opus-4-20250514",
                provider="anthropic",
                reasoning_score=0.98,  # Nouveau mod√®le le plus puissant
                creativity_score=0.95,
                speed_score=0.75,
                multimodal=True,
                max_tokens=200000,
                cost_per_1k_tokens=0.015,  # $15/MTok input
                specialties=[
                    "superior_reasoning",
                    "complex_problem_solving",
                    "advanced_coding",
                    "multimodal_analysis",
                ],
                last_updated="2025-05-14",
            ),
            "claude-sonnet-4-20250514": ModelCapabilities(
                name="claude-sonnet-4-20250514",
                provider="anthropic",
                reasoning_score=0.96,
                creativity_score=0.93,
                speed_score=0.88,
                multimodal=True,
                max_tokens=200000,
                cost_per_1k_tokens=0.003,  # $3/MTok input
                specialties=[
                    "high_performance",
                    "exceptional_reasoning",
                    "efficiency",
                    "balanced_tasks",
                ],
                last_updated="2025-05-14",
            ),
            "claude-3-7-sonnet-20250219": ModelCapabilities(
                name="claude-3-7-sonnet-20250219",
                provider="anthropic",
                reasoning_score=0.94,
                creativity_score=0.91,
                speed_score=0.90,
                multimodal=True,
                max_tokens=200000,
                cost_per_1k_tokens=0.003,
                specialties=[
                    "extended_thinking",
                    "high_performance",
                    "128k_output",
                    "recent_knowledge",
                ],
                last_updated="2025-02-19",
            ),
            "claude-3-5-sonnet-20241022": ModelCapabilities(
                name="claude-3-5-sonnet-20241022",
                provider="anthropic",
                reasoning_score=0.93,
                creativity_score=0.92,
                speed_score=0.82,
                multimodal=True,
                max_tokens=200000,
                cost_per_1k_tokens=0.003,
                specialties=[
                    "creative_writing",
                    "code_review",
                    "long_context",
                    "ethics",
                ],
                last_updated="2024-10-22",
            ),
            # Google Gemini 2.5 - Derni√®res versions avec thinking
            "gemini-2.5-pro": ModelCapabilities(
                name="gemini-2.5-pro",
                provider="gemini",
                reasoning_score=0.97,  # Nouveau mod√®le le plus puissant de Google
                creativity_score=0.91,
                speed_score=0.82,
                multimodal=True,
                max_tokens=2000000,  # √ânorme contexte
                cost_per_1k_tokens=0.002,
                specialties=[
                    "enhanced_thinking",
                    "maximum_accuracy",
                    "complex_coding",
                    "multimodal_understanding",
                    "large_databases",
                ],
                last_updated="2025-01-01",
            ),
            "gemini-2.5-flash": ModelCapabilities(
                name="gemini-2.5-flash",
                provider="gemini",
                reasoning_score=0.94,
                creativity_score=0.89,
                speed_score=0.95,
                multimodal=True,
                max_tokens=1000000,
                cost_per_1k_tokens=0.0005,
                specialties=[
                    "adaptive_thinking",
                    "cost_efficiency",
                    "high_volume_tasks",
                    "price_performance",
                ],
                last_updated="2025-01-01",
            ),
            "gemini-2.0-flash-exp": ModelCapabilities(
                name="gemini-2.0-flash-exp",
                provider="gemini",
                reasoning_score=0.90,
                creativity_score=0.89,
                speed_score=0.92,
                multimodal=True,
                max_tokens=1000000,
                cost_per_1k_tokens=0.0005,
                specialties=[
                    "multimodal",
                    "large_context",
                    "realtime",
                    "video_analysis",
                ],
                last_updated="2024-12-01",
            ),
            # Grok
            "grok-4": ModelCapabilities(
                name="grok-4",
                provider="grok",
                reasoning_score=0.92,
                creativity_score=0.90,
                speed_score=0.88,
                multimodal=False,
                max_tokens=8192,
                cost_per_1k_tokens=0.0,  # Assuming free for now
                specialties=["realtime", "humor", "unfiltered"],
                last_updated=datetime.now().isoformat(),
            ),
        }

        logger = logging.getLogger(__name__).info(f"üß† Orchestrateur initialis√© avec {len(self.models_db)} mod√®les")

    def _load_from_huggingface(self):
        """Met √† jour la base de mod√®les depuis Hugging Face."""
        try:
            # R√©cup√©rer les mod√®les tendance depuis HF
            hf_url = "https://huggingface.co/api/models"
            params = {
                "sort": "trending",
                "filter": "text-generation",
                "limit": 20,
            }

            _response = requests.get(hf_url, params=params, timeout=10)
            if _response.status_code == 200:
                trending_models = _response.json()
                logger = logging.getLogger(__name__).info(f"üìà R√©cup√©r√© {len(trending_models)} mod√®les tendance HF")

                # Analyser les nouveaux mod√®les pour mise √† jour future
                self._analyze_hf_models(trending_models)

        except Exception as e:
            logger = logging.getLogger(__name__).warning(f"‚ö†Ô∏è Erreur r√©cup√©ration HF: {e}")

    def _analyze_hf_models(self, models: List[Dict]):
        """Analyse les mod√®les HF pour d√©tecter les nouveaux."""
        new_models = []
        for model in models:
            model_id = model.get("id", "")
            if any(
                keyword in model_id.lower()
                for keyword in ["gpt", "claude", "gemini", "llama", "mistral"]
            ):
                new_models.append(
                    {
                        "id": model_id,
                        "downloads": model.get("downloads", 0),
                        "likes": model.get("likes", 0),
                        "created_at": model.get("createdAt"),
                    }
                )

        if new_models:
            logger = logging.getLogger(__name__).info(f"üÜï D√©tect√© {len(new_models)} nouveaux mod√®les potentiels")

    def analyze_task(self, prompt: str, task_type: str = "auto") -> TaskAnalysis:
        """Analyse une t√¢che pour d√©terminer les besoins."""

        # D√©tection automatique du type de t√¢che
        if task_type == "auto":
            task_type = self._detect_task_type(prompt)

        # Analyse de complexit√©
        complexity = self._estimate_complexity(prompt)

        # Besoins en cr√©ativit√©
        creativity_needed = self._estimate_creativity_need(prompt, task_type)

        # Besoins en raisonnement
        reasoning_needed = self._estimate_reasoning_need(prompt, task_type)

        # Priorit√© vitesse
        speed_priority = self._estimate_speed_priority(prompt, task_type)

        # Multimodal n√©cessaire
        multimodal_needed = self._detect_multimodal_need(prompt)

        # Estimation de tokens
        estimated_tokens = len(prompt.split()) * 1.3  # Approximation

        return TaskAnalysis(
            task_type=task_type,
            complexity=complexity,
            creativity_needed=creativity_needed,
            reasoning_needed=reasoning_needed,
            speed_priority=speed_priority,
            multimodal_needed=multimodal_needed,
            estimated_tokens=int(estimated_tokens),
        )

    def _detect_task_type(self, prompt: str) -> str:
        """D√©tecte automatiquement le type de t√¢che."""
        prompt_lower = prompt.lower()

        # Patterns de d√©tection
        if any(
            kw in prompt_lower
            for kw in ["code", "function", "debug", "programming", "script"]
        ):
            return "coding"
        elif any(
            kw in prompt_lower
            for kw in ["math", "calculate", "solve", "equation", "algorithm"]
        ):
            return "mathematical"
        elif any(
            kw in prompt_lower
            for kw in [
                "create",
                "write",
                "story",
                "poem",
                "creative",
                "imagine",
            ]
        ):
            return "creative"
        elif any(
            kw in prompt_lower
            for kw in ["analyze", "reason", "explain", "why", "how", "complex"]
        ):
            return "reasoning"
        elif any(
            kw in prompt_lower
            for kw in ["image", "video", "visual", "picture", "diagram"]
        ):
            return "multimodal"
        elif any(
            kw in prompt_lower for kw in ["quick", "fast", "simple", "brie", "summary"]
        ):
            return "fast"
        else:
            return "general"

    def _estimate_complexity(self, prompt: str) -> float:
        """Estime la complexit√© de la t√¢che (0-1)."""
        factors = []

        # Longueur du prompt
        length_factor = min(len(prompt) / 2000, 1.0)
        factors.append(length_factor)

        # Mots complexes
        complex_words = [
            "algorithm",
            "architecture",
            "optimization",
            "analysis",
            "synthesis",
        ]
        complex_count = sum(1 for word in complex_words if word in prompt.lower())
        complexity_factor = min(complex_count / len(complex_words), 1.0)
        factors.append(complexity_factor)

        # Questions multiples
        question_count = prompt.count("?")
        question_factor = min(question_count / 5, 1.0)
        factors.append(question_factor)

        return sum(factors) / len(factors)

    def _estimate_creativity_need(self, prompt: str, task_type: str) -> float:
        """Estime le besoin en cr√©ativit√© (0-1)."""
        if task_type in ["creative", "writing"]:
            return 0.9
        elif task_type in ["coding", "mathematical"]:
            return 0.3
        elif any(
            kw in prompt.lower()
            for kw in ["creative", "innovative", "original", "unique"]
        ):
            return 0.8
        else:
            return 0.5

    def _estimate_reasoning_need(self, prompt: str, task_type: str) -> float:
        """Estime le besoin en raisonnement (0-1)."""
        if task_type in ["mathematical", "reasoning", "coding"]:
            return 0.9
        elif any(
            kw in prompt.lower() for kw in ["why", "how", "explain", "analyze", "logic"]
        ):
            return 0.8
        else:
            return 0.6

    def _estimate_speed_priority(self, prompt: str, task_type: str) -> float:
        """Estime la priorit√© vitesse (0-1)."""
        if task_type == "fast":
            return 0.9
        elif any(
            kw in prompt.lower()
            for kw in ["quick", "fast", "urgent", "now", "immediately"]
        ):
            return 0.8
        elif any(
            kw in prompt.lower()
            for kw in ["detailed", "thorough", "comprehensive", "deep"]
        ):
            return 0.2
        else:
            return 0.5

    def _detect_multimodal_need(self, prompt: str) -> bool:
        """D√©tecte si la t√¢che n√©cessite du multimodal."""
        multimodal_keywords = [
            "image",
            "picture",
            "photo",
            "video",
            "visual",
            "diagram",
            "chart",
            "graph",
        ]
        return any(kw in prompt.lower() for kw in multimodal_keywords)

    def select_optimal_model(
        self, task_analysis: TaskAnalysis
    ) -> Tuple[str, ModelCapabilities, float]:
        """S√©lectionne le mod√®le optimal pour une t√¢che."""

        best_model = None
        best_score = 0
        best_name = ""

        for model_name, model in self.models_db.items():
            score = self._calculate_model_score(model, task_analysis)

            if score > best_score:
                best_score = score
                best_model = model
                best_name = model_name

        logger = logging.getLogger(__name__).info(
            f"üéØ Mod√®le optimal s√©lectionn√©: {best_name} (score:" "{best_score:.2f})"
        )
        return best_name, best_model, best_score

    def _calculate_model_score(
        self, model: ModelCapabilities, task: TaskAnalysis
    ) -> float:
        """Calcule le score d'ad√©quation mod√®le-t√¢che."""

        scores = []

        # Score raisonnement
        reasoning_score = model.reasoning_score * task.reasoning_needed
        scores.append(reasoning_score)

        # Score cr√©ativit√©
        creativity_score = model.creativity_score * task.creativity_needed
        scores.append(creativity_score)

        # Score vitesse
        speed_score = model.speed_score * task.speed_priority
        scores.append(speed_score)

        # Bonus multimodal si n√©cessaire
        if task.multimodal_needed:
            multimodal_bonus = 0.8 if model.multimodal else 0.0
            scores.append(multimodal_bonus)

        # P√©nalit√© co√ªt pour t√¢ches simples
        if task.complexity < 0.3:
            cost_penalty = max(0, 1 - model.cost_per_1k_tokens * 100)
            scores.append(cost_penalty)

        # Score sp√©cialit√©
        specialty_bonus = 0
        if task.task_type in model.specialties:
            specialty_bonus = 0.2
        scores.append(specialty_bonus)

        # Score final
        final_score = sum(scores) / len(scores)

        # Ajustement historique de performance
        historical_bonus = self._get_historical_performance(model.name)
        final_score += historical_bonus

        return min(final_score, 1.0)

    def _get_historical_performance(self, model_name: str) -> float:
        """R√©cup√®re le bonus de performance historique."""
        # Simul√© pour l'instant - √† impl√©menter avec donn√©es r√©elles
        recent_performances = [
            p for p in self.performance_history if p.get("model") == model_name
        ]

        if recent_performances:
            avg_success = sum(
                p.get("success", 0.5) for p in recent_performances[-10:]
            ) / min(len(recent_performances), 10)
            return (avg_success - 0.5) * 0.1  # Bonus/malus de 10%

        return 0.0

    def record_performance(
        self,
        model_name: str,
        task_type: str,
        success_rate: float,
        latency: float,
        cost: float,
    ):
        """Enregistre la performance d'un mod√®le pour apprentissage."""

        performance = {
            "model": model_name,
            "task_type": task_type,
            "success": success_rate,
            "latency": latency,
            "cost": cost,
            "timestamp": datetime.now().isoformat(),
        }

        self.performance_history.append(performance)

        # Garder seulement les 1000 derni√®res performances
        if len(self.performance_history) > 1000:
            self.performance_history = self.performance_history[-1000:]

        logger = logging.getLogger(__name__).info(
            f"üìä Performance enregistr√©e: {model_name} - {task_type} - "
            f"{success_rate:.2f}"
        )

    def update_models_database(self):
        """Met √† jour la base de donn√©es des mod√®les."""
        try:
            # V√©rifier les nouveaux mod√®les OpenAI
            self._check_openai_updates()

            # V√©rifier les nouveaux mod√®les Anthropic
            self._check_anthropic_updates()

            # V√©rifier les nouveaux mod√®les Gemini
            self._check_gemini_updates()

            # Mise √† jour depuis Hugging Face
            self._load_from_huggingface()

            self.last_update = datetime.now().isoformat()
            logger = logging.getLogger(__name__).info(f"üîÑ Base de mod√®les mise √† jour: {self.last_update}")

        except Exception as e:
            logger = logging.getLogger(__name__).error(f"‚ùå Erreur mise √† jour mod√®les: {e}")

    def _check_openai_updates(self):
        """V√©rifie les nouveaux mod√®les OpenAI."""
        try:
            # Cette API n√©cessite une cl√© OpenAI
            openai_key = os.getenv("OPENAI_API_KEY")
            if not openai_key:
                return

            headers = {"Authorization": f"Bearer {openai_key}"}
            _response = requests.get(
                "https://api.openai.com/v1/models", headers=headers, timeout=10
            )

            if _response.status_code == 200:
                models = _response.json().get("data", [])
                logger = logging.getLogger(__name__).info(f"ü§ñ R√©cup√©r√© {len(models)} mod√®les OpenAI")

                # Analyser pour nouveaux mod√®les GPT-4, o1, etc.
                for model in models:
                    model_id = model.get("id", "")
                    if any(prefix in model_id for prefix in ["gpt-4", "o1-", "gpt-5"]):
                        if model_id not in self.models_db:
                            logger = logging.getLogger(__name__).info(f"üÜï Nouveau mod√®le OpenAI d√©tect√©: {model_id}")

        except Exception as e:
            logger = logging.getLogger(__name__).warning(f"‚ö†Ô∏è Erreur v√©rification OpenAI: {e}")

    def _check_anthropic_updates(self):
        """V√©rifie les nouveaux mod√®les Anthropic."""
        # Anthropic ne fournit pas d'API publique pour lister les mod√®les
        # On peut surveiller leur blog/docs pour les nouveaut√©s
        logger = logging.getLogger(__name__).info("üîç V√©rification manuelle recommand√©e pour nouveaux mod√®les Claude")

    def _check_gemini_updates(self):
        """V√©rifie les nouveaux mod√®les Gemini."""
        try:
            gemini_key = os.getenv("GEMINI_API_KEY")
            if not gemini_key:
                return

            # API Google AI Studio pour lister les mod√®les
            url = (
                "https://generativelanguage.googleapis.com/v1beta/models"
                f"?key={gemini_key}"
            )
            _response = requests.get(url, timeout=10)

            if _response.status_code == 200:
                data = _response.json()
                models = data.get("models", [])
                logger = logging.getLogger(__name__).info(f"üíé R√©cup√©r√© {len(models)} mod√®les Gemini")

                for model in models:
                    model_name = model.get("name", "").replace("models/", "")
                    if "gemini" in model_name and model_name not in self.models_db:
                        logger = logging.getLogger(__name__).info(f"üÜï Nouveau mod√®le Gemini d√©tect√©: {model_name}")

        except Exception as e:
            logger = logging.getLogger(__name__).warning(f"‚ö†Ô∏è Erreur v√©rification Gemini: {e}")

    def get_orchestrator_stats(self) -> Dict:
        """Retourne les statistiques de l'orchestrateur."""
        return {
            "total_models": len(self.models_db),
            "performance_history_count": len(self.performance_history),
            "last_update": self.last_update,
            "models_by_provider": {
                provider: len(
                    [m for m in self.models_db.values() if m.provider == provider]
                )
                for provider in set(m.provider for m in self.models_db.values())
            },
            "avg_scores": {
                "reasoning": (
                    sum(m.reasoning_score for m in self.models_db.values())
                    / len(self.models_db)
                ),
                "creativity": (
                    sum(m.creativity_score for m in self.models_db.values())
                    / len(self.models_db)
                ),
                "speed": (
                    sum(m.speed_score for m in self.models_db.values())
                    / len(self.models_db)
                ),
            },
        }


# Instance globale de l'orchestrateur
_orchestrator_instance = None


def get_orchestrator() -> IntelligentOrchestrator:
    """Retourne l'instance globale de l'orchestrateur."""
    global _orchestrator_instance
    if _orchestrator_instance is None:
        _orchestrator_instance = IntelligentOrchestrator()
    return _orchestrator_instance


def reset_orchestrator():
    """Remet √† z√©ro l'instance globale."""
    global _orchestrator_instance
    _orchestrator_instance = None


__all__ = [
    "IntelligentOrchestrator",
    "TaskAnalysis",
    "ModelCapabilities",
    "get_orchestrator",
    "reset_orchestrator",
]


if __name__ == "__main__":
    # Test de l'orchestrateur
    orchestrator = get_orchestrator()

    # Test d'analyse de t√¢che
    test_prompts = [
        ("√âcris une fonction Python pour trier une liste", "coding"),
        (
            "R√©sous cette √©quation complexe: x^3 + 2x^2 - 5x + 1 = 0",
            "mathematical",
        ),
        ("√âcris un po√®me sur l'intelligence artificielle", "creative"),
        ("Analyse cette image et d√©cris ce que tu vois", "multimodal"),
        ("Donne-moi un r√©sum√© rapide", "fast"),
    ]

    for prompt, expected_type in test_prompts:
        task_analysis = orchestrator.analyze_task(prompt)
        model_name, model, score = orchestrator.select_optimal_model(task_analysis)

        print(f"\nüìù Prompt: {prompt[:50]}...")
        print(f"üéØ Type d√©tect√©: {task_analysis.task_type}")
        print(f"ü§ñ Mod√®le s√©lectionn√©: {model_name}")
        print(f"üìä Score: {score:.2f}")
        print(f"üîß Sp√©cialit√©s: {model.specialties}")

    # Stats
    stats = orchestrator.get_orchestrator_stats()
    print("\nüìä Stats orchestrateur:")
    print(json.dumps(stats, indent=2))
